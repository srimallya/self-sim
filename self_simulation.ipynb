{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srimallya/self-sim/blob/main/self_simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cognitive AI Architecture: Parallels with Human Cognition\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This paper presents a novel artificial intelligence architecture that draws inspiration from and parallels with human cognitive functions. We propose a multi-model system where M0 corresponds to language processing, M1 to cognition, M2 to subconscious processes, M3 to decision-making, and additional components mirroring dopamine (epsilon), self (window size), and the limbic system (energy gradient). This architecture demonstrates remarkable similarities to human cognitive processes and offers new insights into both artificial intelligence design and our understanding of human cognition.\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Recent advancements in artificial intelligence have increasingly looked to human cognition for inspiration. This paper presents an AI architecture that not only draws from cognitive science but also offers a framework for understanding human cognitive processes. By mapping AI components to cognitive functions, we create a bidirectional bridge between AI and cognitive science.\n",
        "\n",
        "## 2. Architecture Overview\n",
        "\n",
        "Our proposed architecture consists of the following key components:\n",
        "\n",
        "1. M0: Language Model\n",
        "2. M1: Cognitive Model\n",
        "3. M2: Subconscious Model\n",
        "4. M3: Decision-Making Model\n",
        "5. Epsilon Model: Analogous to dopamine function\n",
        "6. Window Size Model: Representing the concept of self\n",
        "7. Energy Gradient: Paralleling the limbic system\n",
        "\n",
        "## 3. Detailed Component Analysis\n",
        "\n",
        "### 3.1 M0: Language Model\n",
        "\n",
        "The M0 model, corresponding to language processing, is implemented as a bidirectional LSTM network. This model processes and generates categorical information, mirroring the human ability to understand and produce language.\n",
        "\n",
        "Key features:\n",
        "- Bidirectional processing, allowing for context-aware language understanding\n",
        "- Categorical output, similar to human language's discrete nature\n",
        "- Continuous learning from interaction, reflecting language acquisition processes\n",
        "\n",
        "### 3.2 M1: Cognitive Model\n",
        "\n",
        "M1, representing cognition, is also implemented as a bidirectional LSTM. This model processes a wide range of inputs, including perceptions, energy levels, and past actions, to predict future states.\n",
        "\n",
        "Key features:\n",
        "- Integration of multiple input types, mirroring the multi-modal nature of human cognition\n",
        "- Predictive capabilities, reflecting human cognitive abilities to anticipate future events\n",
        "- Adaptive learning, similar to human cognitive flexibility\n",
        "\n",
        "### 3.3 M2: Subconscious Model\n",
        "\n",
        "The M2 model, analogous to subconscious processes, operates on the combined outputs of M0 and M1 to predict energy gradients. This mirrors the human subconscious's role in processing complex information outside of conscious awareness.\n",
        "\n",
        "Key features:\n",
        "- Processing of high-level cognitive and linguistic inputs\n",
        "- Output influencing motivation and behavior, similar to subconscious effects on human actions\n",
        "- Continuous background operation, paralleling the constant activity of the human subconscious\n",
        "\n",
        "### 3.4 M3: Decision-Making Model\n",
        "\n",
        "M3, implemented as a Q-learning model with a bidirectional LSTM, represents the decision-making process. It integrates inputs from all other models to determine actions.\n",
        "\n",
        "Key features:\n",
        "- Reinforcement learning approach, similar to human learning from consequences\n",
        "- Integration of conscious (M1) and subconscious (M2) inputs, mirroring human decision-making\n",
        "- Adaptive action selection, reflecting human behavioral flexibility\n",
        "\n",
        "### 3.5 Epsilon Model: The Dopamine Analogue\n",
        "\n",
        "The epsilon model, representing the function of dopamine in the brain, balances exploration and exploitation in decision-making.\n",
        "\n",
        "Key features:\n",
        "- Modulation of exploration vs. exploitation, similar to dopamine's role in reward-seeking behavior\n",
        "- Adaptation based on past performance and predicted future rewards, mirroring dopamine's role in learning\n",
        "\n",
        "### 3.6 Window Size Model: The Concept of Self\n",
        "\n",
        "The window size model, analogous to the concept of self, determines the temporal scope of experiences considered in decision-making.\n",
        "\n",
        "Key features:\n",
        "- Adaptive integration of past experiences, similar to human autobiographical memory\n",
        "- Influence on other components, reflecting the pervasive impact of self in human cognition\n",
        "\n",
        "### 3.7 Energy Gradient: The Limbic System Analogue\n",
        "\n",
        "The energy gradient, paralleling the limbic system, guides the agent towards rewards in the environment.\n",
        "\n",
        "Key features:\n",
        "- Motivation of goal-directed behavior, similar to the limbic system's role in emotion and motivation\n",
        "- Integration with other components, reflecting the interplay of emotion and cognition in human decision-making\n",
        "\n",
        "## 4. Interactions and Emergent Behaviors\n",
        "\n",
        "The interactions between these components lead to emergent behaviors that closely resemble human cognitive processes:\n",
        "\n",
        "1. Language influencing cognition (M0 → M1): Mimics how human language shapes thought processes.\n",
        "2. Cognition affecting subconscious (M1 → M2): Reflects how conscious thoughts can influence subconscious processes.\n",
        "3. Subconscious guiding decisions (M2 → M3): Parallels the significant role of subconscious processes in human decision-making.\n",
        "4. Self-concept modulating dopamine (Window Size → Epsilon): Mirrors how self-perception influences motivation and reward-seeking behavior.\n",
        "5. Dopamine affecting the limbic system (Epsilon → Energy Gradient): Reflects the interplay between reward prediction and emotional responses.\n",
        "\n",
        "## 5. Implications and Future Directions\n",
        "\n",
        "This architecture provides several important implications for both AI development and cognitive science:\n",
        "\n",
        "1. It offers a new framework for developing more human-like AI systems, potentially leading to more intuitive and effective AI interactions.\n",
        "2. It provides a computational model for testing theories of human cognition, allowing for simulations of cognitive processes.\n",
        "3. The architecture suggests new hypotheses about the relationships between language, consciousness, and decision-making in human cognition.\n",
        "\n",
        "Future work should focus on:\n",
        "1. Refining the models to more closely match neurobiological findings.\n",
        "2. Expanding the architecture to include other cognitive functions, such as memory and attention.\n",
        "3. Developing applications in fields like cognitive robotics and advanced AI assistants.\n",
        "\n",
        "## 6. Conclusion\n",
        "\n",
        "The proposed cognitive AI architecture offers a promising bridge between artificial intelligence and cognitive science. By mapping AI components to cognitive functions, we create a system that not only performs well as an AI but also provides insights into human cognition. This bidirectional flow of ideas between AI and cognitive science opens new avenues for research and development in both fields.\n"
      ],
      "metadata": {
        "id": "no5y1NJgUtA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## deep net bi-lstm + taxis + Q-lstm ## dopamine + self model ## category trade ## v25. ##pacman ## negative reward % ## multi states ## rerun with viz\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pygame\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import warnings\n",
        "import joblib\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Constants\n",
        "GRID_SIZE = 22\n",
        "CELL_SIZE = 20\n",
        "SCREEN_SIZE = GRID_SIZE * CELL_SIZE\n",
        "TIME_STEPS = 2000\n",
        "N_AGENTS = 2\n",
        "COLORS = ['cyan', 'yellow']\n",
        "CONSTANT_FOOD_COUNT = 50\n",
        "FOOD_ENERGY_RANGE = (100, 200)\n",
        "MAX_ENERGY = 1000\n",
        "ENERGY_LOSS_MOVE = 1.0\n",
        "ENERGY_LOSS_STATIONARY = 0.1\n",
        "ENERGY_GAIN_FOOD = 20\n",
        "PERCEPTION_WINDOW = 180\n",
        "PERCEPTION_RANGE = 8\n",
        "COLLISION_PENALTY = 1.0\n",
        "M3_SEQUENCE_LENGTH = 50\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Maze layout (1 for walls, 0 for paths)\n",
        "MAZE = np.array([\n",
        "    [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1],\n",
        "    [1,0,1,1,1,1,0,0,0,0,0,0,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,1,1,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "    [1,0,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1],\n",
        "    [1,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1],\n",
        "    [1,0,1,1,1,0,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1],\n",
        "    [1,1,1,0,1,0,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1],\n",
        "    [1,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1],\n",
        "    [1,0,1,1,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "    [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1]\n",
        "])\n",
        "\n",
        "class SharedCategorySpace:\n",
        "    def __init__(self):\n",
        "        self.shared_categories = {}\n",
        "\n",
        "    def update(self, agent_id, categories):\n",
        "        self.shared_categories[agent_id] = categories\n",
        "\n",
        "    def get_shared_categories(self):\n",
        "        return self.shared_categories\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, x, y, color):\n",
        "        self.pos = np.array([x, y])\n",
        "        self.energy = MAX_ENERGY / 2\n",
        "        self.initial_energy = self.energy\n",
        "        self.previous_energy = self.energy\n",
        "        self.angle = random.uniform(0, 360)\n",
        "        self.color = color\n",
        "        self.energy_tracking = [0] * 10\n",
        "        self.movement_tracking = [np.zeros(2)] * 10\n",
        "        self.angle_tracking = [0] * 10\n",
        "        self.perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.raw_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.previous_raw_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.energy_gradient = np.zeros(360)\n",
        "        self.raw_energy_gradient = np.zeros(360)\n",
        "        self.predicted_energy_gradient = np.zeros(360)\n",
        "        self.generated_perception_M0 = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.future_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "\n",
        "        self.M0 = self._create_bidirectional_lstm_model((10, PERCEPTION_RANGE * PERCEPTION_WINDOW), PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "        total_input_size = (PERCEPTION_RANGE * PERCEPTION_WINDOW) + 360 + (2 * 10) + 10 + 10\n",
        "        self.M1 = self._create_bidirectional_lstm_model((10, total_input_size), PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "        m2_input_size = PERCEPTION_RANGE * PERCEPTION_WINDOW * 2\n",
        "        self.M2 = self._create_bidirectional_lstm_model((10, m2_input_size), 360)\n",
        "\n",
        "        self.M0_errors = []\n",
        "        self.M1_errors = []\n",
        "        self.M2_errors = []\n",
        "        self.M3_errors = []\n",
        "\n",
        "        self.categories = None\n",
        "        self.received_categories = None\n",
        "\n",
        "        self.rotation_speed = 60\n",
        "        self.locomotion_steps = 0\n",
        "        self.max_locomotion_steps = 10\n",
        "        self.min_observation_steps = 1\n",
        "        self.observation_steps = 0\n",
        "\n",
        "        self.energy_loss_move = ENERGY_LOSS_MOVE\n",
        "        self.energy_loss_stationary = ENERGY_LOSS_STATIONARY\n",
        "        self.energy_gain_food = ENERGY_GAIN_FOOD\n",
        "\n",
        "        self.stuck_counter = 0\n",
        "        self.last_position = self.pos.copy()\n",
        "        self.is_randomized = False\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.cumulative_efficiency = 0\n",
        "\n",
        "        self.M3_sequence_length = M3_SEQUENCE_LENGTH\n",
        "        self.M3_state_size = PERCEPTION_RANGE * PERCEPTION_WINDOW + 360 + 1\n",
        "        self.M3_action_size = 20\n",
        "        self.M3 = self._create_bidirectional_lstm_q_model()\n",
        "        self.M3_sequence = [np.zeros(self.M3_state_size)] * self.M3_sequence_length\n",
        "\n",
        "        self.M0_sequence = []\n",
        "        self.M1_sequence = []\n",
        "        self.M2_sequence = []\n",
        "\n",
        "        self.last_action_M3 = None\n",
        "        self.gradient_choices = [0] * self.M3_action_size\n",
        "\n",
        "        self.epsilon = 0.5\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_learning_rate = 0.01\n",
        "        self.energy_window = []\n",
        "        self.energy_window_size = 10\n",
        "        self.min_window_size = 5\n",
        "        self.max_window_size = 50\n",
        "        self.epsilon_model = self._create_epsilon_model()\n",
        "\n",
        "        self.window_size_model = self._create_window_size_model()\n",
        "        self.loss_window = []\n",
        "        self.loss_window_size = 10\n",
        "\n",
        "        self.efficiency_score = 0\n",
        "        self.min_efficiency = float('inf')\n",
        "        self.max_efficiency = float('-inf')\n",
        "\n",
        "    def _create_bidirectional_lstm_model(self, input_shape, output_size):\n",
        "        model = Sequential([\n",
        "            Bidirectional(LSTM(16, return_sequences=True), input_shape=input_shape),\n",
        "            Bidirectional(LSTM(16, return_sequences=True)),\n",
        "            Bidirectional(LSTM(16)),\n",
        "            Dense(output_size)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.05), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _create_bidirectional_lstm_q_model(self):\n",
        "        model = Sequential([\n",
        "            Bidirectional(LSTM(128, return_sequences=True, input_shape=(self.M3_sequence_length, self.M3_state_size))),\n",
        "            Bidirectional(LSTM(64, return_sequences=True)),\n",
        "            Bidirectional(LSTM(32, return_sequences=True)),\n",
        "            Bidirectional(LSTM(16)),\n",
        "            Dense(self.M3_action_size)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _create_epsilon_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(32, activation='relu', input_shape=(2,)),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _create_window_size_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(16, activation='relu', input_shape=(3,)),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _get_m3_state(self):\n",
        "        return np.concatenate([\n",
        "            self.perception.flatten(),\n",
        "            self.energy_gradient,\n",
        "            [self.energy]\n",
        "        ])\n",
        "\n",
        "    def perceive(self, food_positions, agents, shared_category_space):\n",
        "        self.previous_raw_perception = self.raw_perception.copy()\n",
        "        self.raw_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.received_categories = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "\n",
        "        for angle in range(PERCEPTION_WINDOW):\n",
        "            for distance in range(1, PERCEPTION_RANGE + 1):\n",
        "                x = int(self.pos[0] + distance * math.cos(math.radians(self.angle + angle - PERCEPTION_WINDOW/2)))\n",
        "                y = int(self.pos[1] + distance * math.sin(math.radians(self.angle + angle - PERCEPTION_WINDOW/2)))\n",
        "                x, y = x % GRID_SIZE, y % GRID_SIZE\n",
        "\n",
        "                if MAZE[y, x] == 1:\n",
        "                    self.raw_perception[distance-1, angle] = -1\n",
        "                    self.perception[distance-1, angle] = -1\n",
        "                    break\n",
        "\n",
        "                if (x, y) in food_positions:\n",
        "                    self.raw_perception[distance-1, angle] = food_positions[(x, y)] / distance\n",
        "                    self.perception[distance-1, angle] = food_positions[(x, y)] / distance\n",
        "\n",
        "                for agent in agents:\n",
        "                    if agent != self and np.array_equal(agent.pos, [x, y]):\n",
        "                        self.raw_perception[distance-1, angle] = -agent.energy / distance\n",
        "                        self.perception[distance-1, angle] = -agent.energy / distance\n",
        "                        if agent.color in shared_category_space.get_shared_categories():\n",
        "                            self.received_categories[distance-1, angle] = shared_category_space.get_shared_categories()[agent.color][distance-1 * PERCEPTION_WINDOW + angle]\n",
        "\n",
        "        self.raw_energy_gradient = self._calculate_energy_gradient(food_positions)\n",
        "        if not self.is_randomized:\n",
        "            self.energy_gradient = self.raw_energy_gradient.copy()\n",
        "\n",
        "        self.M0_sequence.append(self.perception.flatten())\n",
        "        if len(self.M0_sequence) > 10:\n",
        "            self.M0_sequence.pop(0)\n",
        "\n",
        "        if len(self.M0_sequence) == 10:\n",
        "            X_M0 = np.array(self.M0_sequence).reshape(1, 10, -1)\n",
        "            self.categories = self.M0.predict(X_M0, verbose=0)[0]\n",
        "        else:\n",
        "            self.categories = np.zeros(PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "\n",
        "        shared_category_space.update(self.color, self.categories)\n",
        "\n",
        "        M1_input = np.hstack((\n",
        "            self.raw_perception.flatten(),\n",
        "            self.raw_energy_gradient,\n",
        "            np.array(self.movement_tracking).flatten(),\n",
        "            np.array(self.energy_tracking),\n",
        "            np.array(self.angle_tracking)\n",
        "        ))\n",
        "        self.M1_sequence.append(M1_input)\n",
        "        if len(self.M1_sequence) > 10:\n",
        "            self.M1_sequence.pop(0)\n",
        "\n",
        "        if len(self.M1_sequence) == 10:\n",
        "            X_M1 = np.array(self.M1_sequence).reshape(1, 10, -1)\n",
        "            self.future_perception = self.M1.predict(X_M1, verbose=0)[0]\n",
        "        else:\n",
        "            self.future_perception = np.zeros(PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "\n",
        "        M2_input = np.hstack((self.raw_perception.flatten(), self.future_perception))\n",
        "        self.M2_sequence.append(M2_input)\n",
        "        if len(self.M2_sequence) > 10:\n",
        "            self.M2_sequence.pop(0)\n",
        "\n",
        "        if len(self.M2_sequence) == 10:\n",
        "            X_M2 = np.array(self.M2_sequence).reshape(1, 10, -1)\n",
        "            self.predicted_energy_gradient = self.M2.predict(X_M2, verbose=0)[0]\n",
        "        else:\n",
        "            self.predicted_energy_gradient = np.zeros(360)\n",
        "\n",
        "        m3_state = self._get_m3_state()\n",
        "        self.M3_sequence.append(m3_state)\n",
        "        if len(self.M3_sequence) > self.M3_sequence_length:\n",
        "            self.M3_sequence.pop(0)\n",
        "\n",
        "        return self.perception, self.future_perception\n",
        "\n",
        "    def _calculate_energy_gradient(self, food_positions):\n",
        "        energy_gradient = np.zeros(360)\n",
        "        for angle in range(360):\n",
        "            energy = 0\n",
        "            for distance in range(1, PERCEPTION_RANGE + 1):\n",
        "                x = int(self.pos[0] + distance * math.cos(math.radians(angle)))\n",
        "                y = int(self.pos[1] + distance * math.sin(math.radians(angle)))\n",
        "                x, y = x % GRID_SIZE, y % GRID_SIZE\n",
        "\n",
        "                if MAZE[y, x] == 1:\n",
        "                    break\n",
        "\n",
        "                if (x, y) in food_positions:\n",
        "                    energy += food_positions[(x, y)] / distance\n",
        "\n",
        "            energy_gradient[angle] = energy\n",
        "        return energy_gradient\n",
        "\n",
        "    def move(self, food_positions, agents, shared_category_space):\n",
        "        self.step_count += 1\n",
        "        previous_pos = np.array(self.pos)\n",
        "        previous_angle = self.angle\n",
        "        self.previous_energy = self.energy\n",
        "\n",
        "        self.perceive(food_positions, agents, shared_category_space)\n",
        "\n",
        "        action_M3 = self._choose_action()\n",
        "\n",
        "        if self.locomotion_steps >= self.max_locomotion_steps:\n",
        "            energy_expenditure = self._pause_and_observe()\n",
        "            self.observation_steps += 1\n",
        "            if self.observation_steps >= self.min_observation_steps:\n",
        "                self.locomotion_steps = 0\n",
        "        else:\n",
        "            self.locomotion_steps += 1\n",
        "            self.observation_steps = 0\n",
        "\n",
        "            energy_expenditure = self._locomotion_taxis(self.energy_gradient)\n",
        "            self.gradient_choices[action_M3] += 1\n",
        "\n",
        "        if tuple(self.pos) in food_positions:\n",
        "            energy_gain = self.energy_gain_food\n",
        "            self.energy += energy_gain\n",
        "            del food_positions[tuple(self.pos)]\n",
        "            if self.is_randomized:\n",
        "                self.is_randomized = False\n",
        "                self.stuck_counter = 0\n",
        "\n",
        "        self.energy -= energy_expenditure\n",
        "\n",
        "        reward = self._calculate_reward()\n",
        "\n",
        "        movement = self.pos - previous_pos\n",
        "        angle_change = (self.angle - previous_angle + 180) % 360 - 180\n",
        "\n",
        "        self.track_energy_movement_angle(energy_expenditure, movement, angle_change)\n",
        "\n",
        "        self.last_action_M3 = action_M3\n",
        "\n",
        "        if np.array_equal(self.pos, self.last_position):\n",
        "            self.stuck_counter += 1\n",
        "            if self.stuck_counter >= 20:\n",
        "                self._randomize_energy_gradient()\n",
        "                self.is_randomized = True\n",
        "        else:\n",
        "            if not self.is_randomized:\n",
        "                self.stuck_counter = 0\n",
        "\n",
        "        self.last_position = self.pos.copy()\n",
        "\n",
        "        self.update_efficiency_score()\n",
        "        self.update_efficiency_bounds()\n",
        "        self.epsilon = self.generate_epsilon()\n",
        "\n",
        "    def _choose_action(self):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.M3_action_size - 1)\n",
        "        else:\n",
        "            if len(self.M3_sequence) == self.M3_sequence_length:\n",
        "                m3_input = np.array(self.M3_sequence).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "                q_values = self.M3.predict(m3_input, verbose=0)[0]\n",
        "                return np.argmax(q_values)\n",
        "            else:\n",
        "                return random.randint(0, self.M3_action_size - 1)\n",
        "\n",
        "    def _locomotion_taxis(self, energy_gradient):\n",
        "        m3_input = np.array(self.M3_sequence).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "        q_values = self.M3.predict(m3_input, verbose=0)[0]\n",
        "\n",
        "        normalized_q_values = (q_values - np.min(q_values)) / (np.max(q_values) - np.min(q_values) + 1e-8)\n",
        "\n",
        "        random_gradient = np.random.rand(360)\n",
        "\n",
        "        combined_gradient = (\n",
        "            energy_gradient * (1 - self.epsilon) * 0.4 +\n",
        "            random_gradient * self.epsilon * 0.6 +\n",
        "            self._q_values_to_gradient(normalized_q_values) * (1 - self.epsilon) * 0.6\n",
        "        )\n",
        "\n",
        "        target_angle = np.argmax(combined_gradient)\n",
        "\n",
        "        angle_diff = (target_angle - self.angle + 180) % 360 - 180\n",
        "        adjustment = min(abs(angle_diff), self.rotation_speed) * np.sign(angle_diff)\n",
        "        self.angle = (self.angle + adjustment) % 360\n",
        "\n",
        "        move_x = int(round(math.cos(math.radians(self.angle))))\n",
        "        move_y = int(round(math.sin(math.radians(self.angle))))\n",
        "\n",
        "        return self._apply_movement(move_x, move_y)\n",
        "\n",
        "    def _q_values_to_gradient(self, q_values):\n",
        "        gradient = np.zeros(360)\n",
        "        for i, q in enumerate(q_values):\n",
        "            angle = (i / len(q_values)) * 360\n",
        "            gradient[int(angle)] = q\n",
        "        return gradient\n",
        "\n",
        "    def _pause_and_observe(self):\n",
        "        target_angle = np.argmax(self.energy_gradient)\n",
        "        angle_diff = (target_angle - self.angle + 180) % 360 - 180\n",
        "        adjustment = min(abs(angle_diff), self.rotation_speed) * np.sign(angle_diff)\n",
        "        self.angle = (self.angle + adjustment) % 360\n",
        "        return self.energy_loss_stationary\n",
        "\n",
        "    def _apply_movement(self, move_x, move_y):\n",
        "        energy_expenditure = 0\n",
        "        steps = max(abs(move_x), abs(move_y))\n",
        "        for _ in range(steps):\n",
        "            step_x = np.sign(move_x) if move_x != 0 else 0\n",
        "            step_y = np.sign(move_y) if move_y != 0 else 0\n",
        "            new_x = int((self.pos[0] + step_x) % GRID_SIZE)\n",
        "            new_y = int((self.pos[1] + step_y) % GRID_SIZE)\n",
        "            if MAZE[new_y, new_x] == 0:  # Check if the new position is not a wall\n",
        "                self.pos = np.array([new_x, new_y])\n",
        "                energy_expenditure += self.energy_loss_move\n",
        "                move_x -= step_x\n",
        "                move_y -= step_y\n",
        "            else:\n",
        "                # If it's a wall, add collision penalty and stop movement\n",
        "                energy_expenditure += self.energy_loss_stationary + COLLISION_PENALTY\n",
        "                break\n",
        "\n",
        "        return energy_expenditure\n",
        "\n",
        "    def track_energy_movement_angle(self, energy_expenditure, movement, angle_change):\n",
        "        self.energy_tracking.append(-energy_expenditure)  # Negative value for expenditure\n",
        "        self.energy_tracking = self.energy_tracking[-10:]\n",
        "\n",
        "        self.movement_tracking.append(movement)\n",
        "        self.movement_tracking = self.movement_tracking[-10:]\n",
        "\n",
        "        self.angle_tracking.append(angle_change)\n",
        "        self.angle_tracking = self.angle_tracking[-10:]\n",
        "\n",
        "    def _randomize_energy_gradient(self):\n",
        "        self.energy_gradient = np.random.rand(360)\n",
        "        self.predicted_energy_gradient = self.energy_gradient.copy()\n",
        "\n",
        "    def update_efficiency_score(self):\n",
        "        energy_change = self.energy - self.initial_energy\n",
        "        self.efficiency_score = energy_change / max(1, self.step_count)\n",
        "\n",
        "    def get_efficiency_score(self):\n",
        "        return self.efficiency_score\n",
        "\n",
        "    def update_efficiency_bounds(self):\n",
        "        self.min_efficiency = min(self.min_efficiency, self.efficiency_score)\n",
        "        self.max_efficiency = max(self.max_efficiency, self.efficiency_score)\n",
        "\n",
        "    def generate_epsilon(self):\n",
        "        normalized_score = (self.efficiency_score - self.min_efficiency) / (self.max_efficiency - self.min_efficiency + 1e-8)\n",
        "        new_epsilon = 1 - normalized_score\n",
        "        return max(self.epsilon_min, min(self.epsilon_max, new_epsilon))\n",
        "\n",
        "    def optimize_epsilon(self, food_positions, agents, shared_category_space):\n",
        "        current_efficiency = self.get_efficiency_score()\n",
        "\n",
        "        # Try slightly higher epsilon\n",
        "        original_epsilon = self.epsilon\n",
        "        self.epsilon = min(self.epsilon * 1.1, self.epsilon_max)\n",
        "        self.move(food_positions, agents, shared_category_space)\n",
        "        higher_efficiency = self.get_efficiency_score()\n",
        "\n",
        "        # Reset and try slightly lower epsilon\n",
        "        self.epsilon = original_epsilon\n",
        "        self.move(food_positions, agents, shared_category_space)  # Reset state\n",
        "        self.epsilon = max(self.epsilon * 0.9, self.epsilon_min)\n",
        "        self.move(food_positions, agents, shared_category_space)\n",
        "        lower_efficiency = self.get_efficiency_score()\n",
        "\n",
        "        # Choose the best epsilon\n",
        "        if higher_efficiency > current_efficiency and higher_efficiency > lower_efficiency:\n",
        "            self.epsilon = min(self.epsilon * 1.1, self.epsilon_max)\n",
        "        elif lower_efficiency > current_efficiency and lower_efficiency > higher_efficiency:\n",
        "            self.epsilon = max(self.epsilon * 0.9, self.epsilon_min)\n",
        "        else:\n",
        "            self.epsilon = original_epsilon\n",
        "\n",
        "    def train_models(self, food_positions, agents, shared_category_space):\n",
        "        current_losses = []\n",
        "\n",
        "        if len(self.M0_sequence) == 10:\n",
        "            X_M0 = np.array(self.M0_sequence).reshape(1, 10, PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "            y_M0 = self.received_categories.flatten().reshape(1, -1)\n",
        "            history = self.M0.fit(X_M0, y_M0, epochs=1, verbose=0)\n",
        "            self.M0_errors.append(history.history['loss'][0])\n",
        "            current_losses.append(history.history['loss'][0])\n",
        "\n",
        "        if len(self.M1_sequence) == 10:\n",
        "            X_M1 = np.array(self.M1_sequence).reshape(1, 10, -1)\n",
        "            y_M1 = self.raw_perception.flatten().reshape(1, -1)\n",
        "            history = self.M1.fit(X_M1, y_M1, epochs=1, verbose=0)\n",
        "            self.M1_errors.append(history.history['loss'][0])\n",
        "            current_losses.append(history.history['loss'][0])\n",
        "\n",
        "        if len(self.M2_sequence) == 10:\n",
        "            X_M2 = np.array(self.M2_sequence).reshape(1, 10, -1)\n",
        "            y_M2 = self.raw_energy_gradient.reshape(1, -1)\n",
        "            history = self.M2.fit(X_M2, y_M2, epochs=1, verbose=0)\n",
        "            self.M2_errors.append(history.history['loss'][0])\n",
        "            current_losses.append(history.history['loss'][0])\n",
        "\n",
        "        if len(self.M3_sequence) == self.M3_sequence_length:\n",
        "            m3_input = np.array(self.M3_sequence).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "            m3_target = self.M3.predict(m3_input, verbose=0)\n",
        "\n",
        "            reward = self._calculate_reward()\n",
        "            next_m3_input = np.array(self.M3_sequence[1:] + [self.M3_sequence[-1]]).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "            next_q_values = self.M3.predict(next_m3_input, verbose=0)[0]\n",
        "\n",
        "            m3_target[0, self.last_action_M3] = reward + GAMMA * np.max(next_q_values)\n",
        "\n",
        "            history = self.M3.fit(m3_input, m3_target, epochs=1, verbose=0)\n",
        "            self.M3_errors.append(history.history['loss'][0])\n",
        "\n",
        "        # Update loss window and train window size model\n",
        "        if len(current_losses) == 3:  # We have losses for M0, M1, and M2\n",
        "            self.loss_window.append(current_losses)\n",
        "            if len(self.loss_window) > self.loss_window_size:\n",
        "                self.loss_window.pop(0)\n",
        "\n",
        "            self._update_energy_window_size()\n",
        "            self._train_window_size_model()\n",
        "\n",
        "        # Train epsilon model\n",
        "        self._train_epsilon_model()\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        energy_change = self.energy - self.previous_energy\n",
        "        reward = energy_change * 0.2\n",
        "        if energy_change > 0:\n",
        "            reward += 2\n",
        "        elif energy_change < 0:\n",
        "            reward -= 0.5\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def _update_energy_window_size(self):\n",
        "        if len(self.loss_window) < self.loss_window_size:\n",
        "            return\n",
        "\n",
        "        loss_changes = np.diff(self.loss_window, axis=0)\n",
        "        avg_loss_changes = np.mean(loss_changes, axis=0)\n",
        "\n",
        "        X = np.array([avg_loss_changes])\n",
        "        window_size_change = self.window_size_model.predict(X, verbose=0)[0][0]\n",
        "\n",
        "        self.energy_window_size = int(max(self.min_window_size,\n",
        "                                          min(self.max_window_size,\n",
        "                                              self.energy_window_size + window_size_change)))\n",
        "\n",
        "        while len(self.energy_window) > self.energy_window_size:\n",
        "            self.energy_window.pop(0)\n",
        "\n",
        "    def _train_epsilon_model(self):\n",
        "        if len(self.energy_window) == self.energy_window_size:\n",
        "            energy_trend = np.mean(np.diff(self.energy_window))\n",
        "            normalized_trend = (energy_trend - self.energy_loss_move) / (self.energy_gain_food - self.energy_loss_move)\n",
        "\n",
        "            m3_input = np.array(self.M3_sequence).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "            predicted_future_value = np.max(self.M3.predict(m3_input, verbose=0)[0])\n",
        "\n",
        "            X = np.array([[normalized_trend, predicted_future_value]])\n",
        "            y = np.array([[self.epsilon]])\n",
        "\n",
        "            self.epsilon_model.fit(X, y, epochs=1, verbose=0)\n",
        "\n",
        "    def _train_window_size_model(self):\n",
        "        if len(self.loss_window) < self.loss_window_size:\n",
        "            return\n",
        "\n",
        "        loss_changes = np.diff(self.loss_window, axis=0)\n",
        "        if len(loss_changes) < 2:  # Need at least 2 samples to calculate a meaningful diff\n",
        "            return\n",
        "\n",
        "        X = loss_changes\n",
        "\n",
        "        window_sizes = [len(self.energy_window)] * (len(loss_changes) + 1)\n",
        "        y = np.diff(window_sizes)\n",
        "\n",
        "        assert len(X) == len(y), \"X and y must have the same number of samples\"\n",
        "\n",
        "        self.window_size_model.fit(X, y, epochs=1, verbose=0)\n",
        "\n",
        "    def get_energy_score(self):\n",
        "        return (self.energy - self.initial_energy) / max(1, self.step_count)\n",
        "\n",
        "    def get_gradient_choice_percentages(self):\n",
        "        total_choices = sum(self.gradient_choices)\n",
        "        if total_choices == 0:\n",
        "            return [0] * self.M3_action_size\n",
        "        return [100 * choices / total_choices for choices in self.gradient_choices]\n",
        "\n",
        "    def save_models(self):\n",
        "        self.M0.save(f'M0_{self.color}_model.h5')\n",
        "        self.M1.save(f'M1_{self.color}_model.h5')\n",
        "        self.M2.save(f'M2_{self.color}_model.h5')\n",
        "        self.M3.save(f'M3_{self.color}_model.h5')\n",
        "        self.epsilon_model.save(f'epsilon_{self.color}_model.h5')\n",
        "        self.window_size_model.save(f'window_size_{self.color}_model.h5')\n",
        "\n",
        "    def load_models(self):\n",
        "        try:\n",
        "            self.M0 = tf.keras.models.load_model(f'M0_{self.color}_model.h5')\n",
        "            self.M1 = tf.keras.models.load_model(f'M1_{self.color}_model.h5')\n",
        "            self.M2 = tf.keras.models.load_model(f'M2_{self.color}_model.h5')\n",
        "            self.M3 = tf.keras.models.load_model(f'M3_{self.color}_model.h5')\n",
        "            self.epsilon_model = tf.keras.models.load_model(f'epsilon_{self.color}_model.h5')\n",
        "            self.window_size_model = tf.keras.models.load_model(f'window_size_{self.color}_model.h5')\n",
        "            return True\n",
        "        except (OSError, IOError):\n",
        "            return False\n",
        "\n",
        "    def reset_efficiency_tracking(self):\n",
        "        self.initial_energy = self.energy\n",
        "        self.step_count = 0\n",
        "        self.cumulative_efficiency = 0\n",
        "\n",
        "    def get_model_errors(self):\n",
        "        return {\n",
        "            'M0': self.M0_errors[-1] if self.M0_errors else None,\n",
        "            'M1': self.M1_errors[-1] if self.M1_errors else None,\n",
        "            'M2': self.M2_errors[-1] if self.M2_errors else None,\n",
        "            'M3': self.M3_errors[-1] if self.M3_errors else None\n",
        "        }\n",
        "\n",
        "    def get_epsilon(self):\n",
        "        return self.epsilon\n",
        "\n",
        "    def get_window_size(self):\n",
        "        return self.energy_window_size\n",
        "\n",
        "def draw_arrow(screen, color, start, end):\n",
        "    pygame.draw.line(screen, color, start, end, 2)\n",
        "    rotation = math.degrees(math.atan2(start[1]-end[1], end[0]-start[0]))+90\n",
        "    pygame.draw.polygon(screen, color, ((end[0]+5*math.sin(math.radians(rotation)), end[1]+5*math.cos(math.radians(rotation))), (end[0]+5*math.sin(math.radians(rotation-120)), end[1]+5*math.cos(math.radians(rotation-120))), (end[0]+5*math.sin(math.radians(rotation+120)), end[1]+5*math.cos(math.radians(rotation+120)))))\n",
        "\n",
        "def draw_energy_gradient(screen, agent):\n",
        "    center = (int(agent.pos[0] * CELL_SIZE + CELL_SIZE/2), int(agent.pos[1] * CELL_SIZE + CELL_SIZE/2))\n",
        "\n",
        "    # Use the combined gradient from the _locomotion_taxis method\n",
        "    m3_input = np.array(agent.M3_sequence).reshape(1, agent.M3_sequence_length, agent.M3_state_size)\n",
        "    q_values = agent.M3.predict(m3_input, verbose=0)[0]\n",
        "    normalized_q_values = (q_values - np.min(q_values)) / (np.max(q_values) - np.min(q_values) + 1e-8)\n",
        "\n",
        "    random_gradient = np.random.rand(360)\n",
        "\n",
        "    combined_gradient = (\n",
        "        agent.energy_gradient * (1 - agent.epsilon) * 0.4 +\n",
        "        random_gradient * agent.epsilon * 0.6 +\n",
        "        agent._q_values_to_gradient(normalized_q_values) * (1 - agent.epsilon) * 0.6\n",
        "    )\n",
        "\n",
        "    max_energy = np.max(combined_gradient)\n",
        "    if max_energy > 0:\n",
        "        normalized_gradient = combined_gradient / max_energy\n",
        "        base_color = pygame.Color(agent.color)\n",
        "\n",
        "        for angle in range(0, 360, 10):  # Draw every 10 degrees for better performance\n",
        "            energy = normalized_gradient[angle]\n",
        "            end_x = center[0] + int(50 * energy * math.cos(math.radians(angle)))\n",
        "            end_y = center[1] + int(50 * energy * math.sin(math.radians(angle)))\n",
        "            color = (max(0, min(255, int(base_color.r * energy))),\n",
        "                     max(0, min(255, int(base_color.g * energy))),\n",
        "                     max(0, min(255, int(base_color.b * energy))))\n",
        "            pygame.draw.line(screen, color, center, (end_x, end_y), 1)\n",
        "\n",
        "def main():\n",
        "    pygame.init()\n",
        "    screen = pygame.display.set_mode((SCREEN_SIZE, SCREEN_SIZE))\n",
        "    clock = pygame.time.Clock()\n",
        "\n",
        "    agents = []\n",
        "    shared_category_space = SharedCategorySpace()\n",
        "    for i, color in enumerate(COLORS[:N_AGENTS]):\n",
        "        while True:\n",
        "            x, y = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)\n",
        "            if MAZE[y, x] == 0:\n",
        "                agent = Agent(x, y, color)\n",
        "                if agent.load_models():\n",
        "                    print(f\"Loaded existing models for Agent {i+1} ({color})\")\n",
        "                else:\n",
        "                    print(f\"No existing models found for Agent {i+1} ({color}). Starting with new models.\")\n",
        "                agents.append(agent)\n",
        "                break\n",
        "\n",
        "    # Reset efficiency tracking for all agents\n",
        "    for agent in agents:\n",
        "        agent.reset_efficiency_tracking()\n",
        "\n",
        "    food_positions = {}\n",
        "\n",
        "    def add_food(count):\n",
        "        for _ in range(count):\n",
        "            while True:\n",
        "                x, y = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)\n",
        "                if MAZE[y, x] == 0 and (x, y) not in food_positions:\n",
        "                    food_positions[(x, y)] = random.randint(FOOD_ENERGY_RANGE[0], FOOD_ENERGY_RANGE[1])\n",
        "                    break\n",
        "\n",
        "    # Initialize food\n",
        "    add_food(CONSTANT_FOOD_COUNT)\n",
        "\n",
        "    for t in range(TIME_STEPS):\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                pygame.quit()\n",
        "                return\n",
        "\n",
        "        screen.fill((12, 12, 12))\n",
        "\n",
        "        # Draw maze\n",
        "        for y in range(GRID_SIZE):\n",
        "            for x in range(GRID_SIZE):\n",
        "                if MAZE[y, x] == 1:\n",
        "                    pygame.draw.rect(screen, (32, 32, 32), (x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
        "\n",
        "        for agent in agents:\n",
        "            agent.perceive(food_positions, agents, shared_category_space)\n",
        "            agent.move(food_positions, agents, shared_category_space)\n",
        "            agent.train_models(food_positions, agents, shared_category_space)\n",
        "\n",
        "            # Optimize epsilon every 100 steps\n",
        "            if t % 100 == 0:\n",
        "                agent.optimize_epsilon(food_positions, agents, shared_category_space)\n",
        "\n",
        "        # Maintain constant food count\n",
        "        food_deficit = CONSTANT_FOOD_COUNT - len(food_positions)\n",
        "        if food_deficit > 0:\n",
        "            add_food(food_deficit)\n",
        "\n",
        "        # Draw food\n",
        "        for (x, y), energy in food_positions.items():\n",
        "            pygame.draw.circle(screen, (128, 128, 128), (int(x * CELL_SIZE + CELL_SIZE/2), int(y * CELL_SIZE + CELL_SIZE/2)), 3)\n",
        "\n",
        "        # Draw agents and their energy gradients\n",
        "        for agent in agents:\n",
        "            draw_energy_gradient(screen, agent)\n",
        "            start_pos = (int(agent.pos[0] * CELL_SIZE + CELL_SIZE/2), int(agent.pos[1] * CELL_SIZE + CELL_SIZE/2))\n",
        "            end_pos = (int(start_pos[0] + 10 * math.cos(math.radians(agent.angle))),\n",
        "                       int(start_pos[1] + 10 * math.sin(math.radians(agent.angle))))\n",
        "            draw_arrow(screen, pygame.Color(agent.color), start_pos, end_pos)\n",
        "\n",
        "        pygame.display.flip()\n",
        "        clock.tick(24)\n",
        "\n",
        "        # Print learning progression every 100 time steps\n",
        "        if (t + 1) % 10 == 0:\n",
        "            print(f\"\\nTime step: {t+1}\")\n",
        "            for i, agent in enumerate(agents):\n",
        "                efficiency_score = agent.get_efficiency_score()\n",
        "                gradient_percentages = agent.get_gradient_choice_percentages()\n",
        "                model_errors = agent.get_model_errors()\n",
        "                print(f\"Agent {i+1} ({agent.color}):\")\n",
        "                print(f\"  Efficiency Score: {efficiency_score:.4f}\")\n",
        "                print(f\"  Epsilon: {agent.get_epsilon():.4f}\")\n",
        "                print(f\"  Energy Window Size: {agent.get_window_size()}\")\n",
        "                print(f\"  Gradient Choices:\")\n",
        "                for j, percentage in enumerate(gradient_percentages):\n",
        "                    print(f\"    State {j}: {percentage:.2f}%\")\n",
        "\n",
        "                print(\"  Model Losses:\")\n",
        "                for model, error in model_errors.items():\n",
        "                    print(f\"    {model} Loss: {error if error is not None else 'N/A'}\")\n",
        "\n",
        "    # Print final learning progression and save models\n",
        "    print(\"\\nFinal learning progression:\")\n",
        "    for i, agent in enumerate(agents):\n",
        "        efficiency_score = agent.get_efficiency_score()\n",
        "        gradient_percentages = agent.get_gradient_choice_percentages()\n",
        "        model_errors = agent.get_model_errors()\n",
        "        print(f\"Agent {i+1} ({agent.color}):\")\n",
        "        print(f\"  Final Efficiency Score: {efficiency_score:.4f}\")\n",
        "        print(f\"  Final Epsilon: {agent.get_epsilon():.4f}\")\n",
        "        print(f\"  Final Energy Window Size: {agent.get_window_size()}\")\n",
        "        print(f\"  Final Gradient Choices:\")\n",
        "        for j, percentage in enumerate(gradient_percentages):\n",
        "            print(f\"    State {j}: {percentage:.2f}%\")\n",
        "\n",
        "        print(\"  Final Model Losses:\")\n",
        "        for model, error in model_errors.items():\n",
        "            print(f\"    {model} Loss: {error if error is not None else 'N/A'}\")\n",
        "\n",
        "        # Save models\n",
        "        agent.save_models()\n",
        "        print(f\"  Models saved for Agent {i+1} ({agent.color})\")\n",
        "\n",
        "    pygame.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "i2YrSUUSTKEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## deep net bi-lstm + taxis + Q-lstm ## dopamine + self model ## category trade ## v25.3 ##pacman ## negative reward % ## multi states ## inference with viz\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pygame\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import warnings\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Constants\n",
        "GRID_SIZE = 22\n",
        "CELL_SIZE = 20\n",
        "SCREEN_SIZE = GRID_SIZE * CELL_SIZE\n",
        "N_AGENTS = 2\n",
        "COLORS = ['cyan', 'yellow']\n",
        "CONSTANT_FOOD_COUNT = 50\n",
        "FOOD_ENERGY_RANGE = (100, 200)\n",
        "MAX_ENERGY = 1000\n",
        "ENERGY_LOSS_MOVE = 1.0\n",
        "ENERGY_LOSS_STATIONARY = 0.1\n",
        "ENERGY_GAIN_FOOD = 20\n",
        "PERCEPTION_WINDOW = 180\n",
        "PERCEPTION_RANGE = 8\n",
        "COLLISION_PENALTY = 1.0\n",
        "M3_SEQUENCE_LENGTH = 50\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Maze layout (1 for walls, 0 for paths)\n",
        "MAZE = np.array([\n",
        "    [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1],\n",
        "    [1,0,1,1,1,1,0,0,0,0,0,0,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,1,1,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "    [1,0,1,1,1,1,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1],\n",
        "    [1,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,1],\n",
        "    [1,0,1,1,1,0,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1],\n",
        "    [1,1,1,0,1,0,0,1,1,0,1,1,1,1,1,1,1,1,0,1,0,1],\n",
        "    [1,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1],\n",
        "    [1,0,1,1,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,1,0,1],\n",
        "    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
        "    [1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1]\n",
        "])\n",
        "\n",
        "class SharedCategorySpace:\n",
        "    def __init__(self):\n",
        "        self.shared_categories = {}\n",
        "\n",
        "    def update(self, agent_id, categories):\n",
        "        self.shared_categories[agent_id] = categories\n",
        "\n",
        "    def get_shared_categories(self):\n",
        "        return self.shared_categories\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, x, y, color):\n",
        "        self.pos = np.array([x, y])\n",
        "        self.energy = MAX_ENERGY / 2\n",
        "        self.initial_energy = self.energy\n",
        "        self.previous_energy = self.energy\n",
        "        self.angle = random.uniform(0, 360)\n",
        "        self.color = color\n",
        "        self.energy_tracking = [0] * 10\n",
        "        self.movement_tracking = [np.zeros(2)] * 10\n",
        "        self.angle_tracking = [0] * 10\n",
        "        self.perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.raw_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.previous_raw_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.energy_gradient = np.zeros(360)\n",
        "        self.raw_energy_gradient = np.zeros(360)\n",
        "        self.predicted_energy_gradient = np.zeros(360)\n",
        "        self.generated_perception_M0 = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.future_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "\n",
        "        self.M0 = self._create_bidirectional_lstm_model((10, PERCEPTION_RANGE * PERCEPTION_WINDOW), PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "        total_input_size = (PERCEPTION_RANGE * PERCEPTION_WINDOW) + 360 + (2 * 10) + 10 + 10\n",
        "        self.M1 = self._create_bidirectional_lstm_model((10, total_input_size), PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "        m2_input_size = PERCEPTION_RANGE * PERCEPTION_WINDOW * 2\n",
        "        self.M2 = self._create_bidirectional_lstm_model((10, m2_input_size), 360)\n",
        "\n",
        "        self.M0_errors = []\n",
        "        self.M1_errors = []\n",
        "        self.M2_errors = []\n",
        "        self.M3_errors = []\n",
        "\n",
        "        self.categories = None\n",
        "        self.received_categories = None\n",
        "\n",
        "        self.rotation_speed = 60\n",
        "        self.locomotion_steps = 0\n",
        "        self.max_locomotion_steps = 10\n",
        "        self.min_observation_steps = 1\n",
        "        self.observation_steps = 0\n",
        "\n",
        "        self.energy_loss_move = ENERGY_LOSS_MOVE\n",
        "        self.energy_loss_stationary = ENERGY_LOSS_STATIONARY\n",
        "        self.energy_gain_food = ENERGY_GAIN_FOOD\n",
        "\n",
        "        self.stuck_counter = 0\n",
        "        self.last_position = self.pos.copy()\n",
        "        self.is_randomized = False\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.cumulative_efficiency = 0\n",
        "\n",
        "        self.M3_sequence_length = M3_SEQUENCE_LENGTH\n",
        "        self.M3_state_size = PERCEPTION_RANGE * PERCEPTION_WINDOW + 360 + 1\n",
        "        self.M3_action_size = 20\n",
        "        self.M3 = self._create_bidirectional_lstm_q_model()\n",
        "        self.M3_sequence = [np.zeros(self.M3_state_size)] * self.M3_sequence_length\n",
        "\n",
        "        self.M0_sequence = []\n",
        "        self.M1_sequence = []\n",
        "        self.M2_sequence = []\n",
        "\n",
        "        self.last_action_M3 = None\n",
        "        self.gradient_choices = [0] * self.M3_action_size\n",
        "\n",
        "        self.epsilon = 0.5\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_learning_rate = 0.01\n",
        "        self.energy_window = []\n",
        "        self.energy_window_size = 10\n",
        "        self.min_window_size = 5\n",
        "        self.max_window_size = 50\n",
        "        self.epsilon_model = self._create_epsilon_model()\n",
        "\n",
        "        self.window_size_model = self._create_window_size_model()\n",
        "        self.loss_window = []\n",
        "        self.loss_window_size = 10\n",
        "\n",
        "        self.efficiency_score = 0\n",
        "        self.min_efficiency = float('inf')\n",
        "        self.max_efficiency = float('-inf')\n",
        "\n",
        "    def _create_bidirectional_lstm_model(self, input_shape, output_size):\n",
        "        model = Sequential([\n",
        "            Bidirectional(LSTM(16, return_sequences=True), input_shape=input_shape),\n",
        "            Bidirectional(LSTM(16, return_sequences=True)),\n",
        "            Bidirectional(LSTM(16)),\n",
        "            Dense(output_size)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.05), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _create_bidirectional_lstm_q_model(self):\n",
        "        model = Sequential([\n",
        "            Bidirectional(LSTM(128, return_sequences=True, input_shape=(self.M3_sequence_length, self.M3_state_size))),\n",
        "            Bidirectional(LSTM(64, return_sequences=True)),\n",
        "            Bidirectional(LSTM(32, return_sequences=True)),\n",
        "            Bidirectional(LSTM(16)),\n",
        "            Dense(self.M3_action_size)\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _create_epsilon_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(32, activation='relu', input_shape=(2,)),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _create_window_size_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(16, activation='relu', input_shape=(3,)),\n",
        "            Dense(16, activation='relu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def _get_m3_state(self):\n",
        "        return np.concatenate([\n",
        "            self.perception.flatten(),\n",
        "            self.energy_gradient,\n",
        "            [self.energy]\n",
        "        ])\n",
        "\n",
        "    def perceive(self, food_positions, agents, shared_category_space):\n",
        "        self.previous_raw_perception = self.raw_perception.copy()\n",
        "        self.raw_perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.perception = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "        self.received_categories = np.zeros((PERCEPTION_RANGE, PERCEPTION_WINDOW))\n",
        "\n",
        "        for angle in range(PERCEPTION_WINDOW):\n",
        "            for distance in range(1, PERCEPTION_RANGE + 1):\n",
        "                x = int(self.pos[0] + distance * math.cos(math.radians(self.angle + angle - PERCEPTION_WINDOW/2)))\n",
        "                y = int(self.pos[1] + distance * math.sin(math.radians(self.angle + angle - PERCEPTION_WINDOW/2)))\n",
        "                x, y = x % GRID_SIZE, y % GRID_SIZE\n",
        "\n",
        "                if MAZE[y, x] == 1:\n",
        "                    self.raw_perception[distance-1, angle] = -1\n",
        "                    self.perception[distance-1, angle] = -1\n",
        "                    break\n",
        "\n",
        "                if (x, y) in food_positions:\n",
        "                    self.raw_perception[distance-1, angle] = food_positions[(x, y)] / distance\n",
        "                    self.perception[distance-1, angle] = food_positions[(x, y)] / distance\n",
        "\n",
        "                for agent in agents:\n",
        "                    if agent != self and np.array_equal(agent.pos, [x, y]):\n",
        "                        self.raw_perception[distance-1, angle] = -agent.energy / distance\n",
        "                        self.perception[distance-1, angle] = -agent.energy / distance\n",
        "                        if agent.color in shared_category_space.get_shared_categories():\n",
        "                            self.received_categories[distance-1, angle] = shared_category_space.get_shared_categories()[agent.color][distance-1 * PERCEPTION_WINDOW + angle]\n",
        "\n",
        "        self.raw_energy_gradient = self._calculate_energy_gradient(food_positions)\n",
        "        if not self.is_randomized:\n",
        "            self.energy_gradient = self.raw_energy_gradient.copy()\n",
        "\n",
        "        self.M0_sequence.append(self.perception.flatten())\n",
        "        if len(self.M0_sequence) > 10:\n",
        "            self.M0_sequence.pop(0)\n",
        "\n",
        "        if len(self.M0_sequence) == 10:\n",
        "            X_M0 = np.array(self.M0_sequence).reshape(1, 10, -1)\n",
        "            self.categories = self.M0.predict(X_M0, verbose=0)[0]\n",
        "        else:\n",
        "            self.categories = np.zeros(PERCEPTION_RANGE * PERCEPTION_WINDOW)\n",
        "\n",
        "        shared_category_space.update(self.color, self.categories)\n",
        "\n",
        "        # Ensure both arrays have the same dimensions\n",
        "        flattened_raw_perception = self.raw_perception.flatten()\n",
        "        flattened_future_perception = self.future_perception.flatten()\n",
        "\n",
        "        # Concatenate the flattened arrays\n",
        "        M1_input = np.concatenate((flattened_raw_perception, flattened_future_perception))\n",
        "\n",
        "        self.M2_sequence.append(M1_input)\n",
        "        if len(self.M2_sequence) > 10:\n",
        "            self.M2_sequence.pop(0)\n",
        "\n",
        "        if len(self.M2_sequence) == 10:\n",
        "            X_M2 = np.array(self.M2_sequence).reshape(1, 10, -1)\n",
        "            self.predicted_energy_gradient = self.M2.predict(X_M2, verbose=0)[0]\n",
        "        else:\n",
        "            self.predicted_energy_gradient = np.zeros(360)\n",
        "\n",
        "        m3_state = self._get_m3_state()\n",
        "        self.M3_sequence.append(m3_state)\n",
        "        if len(self.M3_sequence) > self.M3_sequence_length:\n",
        "            self.M3_sequence.pop(0)\n",
        "\n",
        "        return self.perception, self.future_perception\n",
        "\n",
        "    def _calculate_energy_gradient(self, food_positions):\n",
        "        energy_gradient = np.zeros(360)\n",
        "        for angle in range(360):\n",
        "            energy = 0\n",
        "            for distance in range(1, PERCEPTION_RANGE + 1):\n",
        "                x = int(self.pos[0] + distance * math.cos(math.radians(angle)))\n",
        "                y = int(self.pos[1] + distance * math.sin(math.radians(angle)))\n",
        "                x, y = x % GRID_SIZE, y % GRID_SIZE\n",
        "\n",
        "                if MAZE[y, x] == 1:\n",
        "                    break\n",
        "\n",
        "                if (x, y) in food_positions:\n",
        "                    energy += food_positions[(x, y)] / distance\n",
        "\n",
        "            energy_gradient[angle] = energy\n",
        "        return energy_gradient\n",
        "\n",
        "    def move(self, food_positions, agents, shared_category_space):\n",
        "        self.step_count += 1\n",
        "        previous_pos = np.array(self.pos)\n",
        "        previous_angle = self.angle\n",
        "        self.previous_energy = self.energy\n",
        "\n",
        "        self.perceive(food_positions, agents, shared_category_space)\n",
        "\n",
        "        action_M3 = self._choose_action()\n",
        "\n",
        "        if self.locomotion_steps >= self.max_locomotion_steps:\n",
        "            energy_expenditure = self._pause_and_observe()\n",
        "            self.observation_steps += 1\n",
        "            if self.observation_steps >= self.min_observation_steps:\n",
        "                self.locomotion_steps = 0\n",
        "        else:\n",
        "            self.locomotion_steps += 1\n",
        "            self.observation_steps = 0\n",
        "\n",
        "            energy_expenditure = self._locomotion_taxis(self.energy_gradient)\n",
        "            self.gradient_choices[action_M3] += 1\n",
        "\n",
        "        if tuple(self.pos) in food_positions:\n",
        "            energy_gain = self.energy_gain_food\n",
        "            self.energy += energy_gain\n",
        "            del food_positions[tuple(self.pos)]\n",
        "            if self.is_randomized:\n",
        "                self.is_randomized = False\n",
        "                self.stuck_counter = 0\n",
        "\n",
        "        self.energy -= energy_expenditure\n",
        "\n",
        "        reward = self._calculate_reward()\n",
        "\n",
        "        movement = self.pos - previous_pos\n",
        "        angle_change = (self.angle - previous_angle + 180) % 360 - 180\n",
        "\n",
        "        self.track_energy_movement_angle(energy_expenditure, movement, angle_change)\n",
        "\n",
        "        self.last_action_M3 = action_M3\n",
        "\n",
        "        if np.array_equal(self.pos, self.last_position):\n",
        "            self.stuck_counter += 1\n",
        "            if self.stuck_counter >= 20:\n",
        "                self._randomize_energy_gradient()\n",
        "                self.is_randomized = True\n",
        "        else:\n",
        "            if not self.is_randomized:\n",
        "                self.stuck_counter = 0\n",
        "\n",
        "        self.last_position = self.pos.copy()\n",
        "\n",
        "        self.update_efficiency_score()\n",
        "        self.update_efficiency_bounds()\n",
        "        self.epsilon = self.generate_epsilon()\n",
        "\n",
        "    def _choose_action(self):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.M3_action_size - 1)\n",
        "        else:\n",
        "            if len(self.M3_sequence) == self.M3_sequence_length:\n",
        "                m3_input = np.array(self.M3_sequence).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "                q_values = self.M3.predict(m3_input, verbose=0)[0]\n",
        "                return np.argmax(q_values)\n",
        "            else:\n",
        "                return random.randint(0, self.M3_action_size - 1)\n",
        "\n",
        "    def _locomotion_taxis(self, energy_gradient):\n",
        "        m3_input = np.array(self.M3_sequence).reshape(1, self.M3_sequence_length, self.M3_state_size)\n",
        "        q_values = self.M3.predict(m3_input, verbose=0)[0]\n",
        "\n",
        "        normalized_q_values = (q_values - np.min(q_values)) / (np.max(q_values) - np.min(q_values) + 1e-8)\n",
        "\n",
        "        random_gradient = np.random.rand(360)\n",
        "\n",
        "        combined_gradient = (\n",
        "            energy_gradient * (1 - self.epsilon) * 0.4 +\n",
        "            random_gradient * self.epsilon * 0.6 +\n",
        "            self._q_values_to_gradient(normalized_q_values) * (1 - self.epsilon) * 0.6\n",
        "        )\n",
        "\n",
        "        target_angle = np.argmax(combined_gradient)\n",
        "\n",
        "        angle_diff = (target_angle - self.angle + 180) % 360 - 180\n",
        "        adjustment = min(abs(angle_diff), self.rotation_speed) * np.sign(angle_diff)\n",
        "        self.angle = (self.angle + adjustment) % 360\n",
        "\n",
        "        move_x = int(round(math.cos(math.radians(self.angle))))\n",
        "        move_y = int(round(math.sin(math.radians(self.angle))))\n",
        "\n",
        "        return self._apply_movement(move_x, move_y)\n",
        "\n",
        "    def _q_values_to_gradient(self, q_values):\n",
        "        gradient = np.zeros(360)\n",
        "        for i, q in enumerate(q_values):\n",
        "            angle = (i / len(q_values)) * 360\n",
        "            gradient[int(angle)] = q\n",
        "        return gradient\n",
        "\n",
        "    def _pause_and_observe(self):\n",
        "        target_angle = np.argmax(self.energy_gradient)\n",
        "        angle_diff = (target_angle - self.angle + 180) % 360 - 180\n",
        "        adjustment = min(abs(angle_diff), self.rotation_speed) * np.sign(angle_diff)\n",
        "        self.angle = (self.angle + adjustment) % 360\n",
        "        return self.energy_loss_stationary\n",
        "\n",
        "    def _apply_movement(self, move_x, move_y):\n",
        "        energy_expenditure = 0\n",
        "        steps = max(abs(move_x), abs(move_y))\n",
        "        for _ in range(steps):\n",
        "            step_x = np.sign(move_x) if move_x != 0 else 0\n",
        "            step_y = np.sign(move_y) if move_y != 0 else 0\n",
        "            new_x = int((self.pos[0] + step_x) % GRID_SIZE)\n",
        "            new_y = int((self.pos[1] + step_y) % GRID_SIZE)\n",
        "            if MAZE[new_y, new_x] == 0:  # Check if the new position is not a wall\n",
        "                self.pos = np.array([new_x, new_y])\n",
        "                energy_expenditure += self.energy_loss_move\n",
        "                move_x -= step_x\n",
        "                move_y -= step_y\n",
        "            else:\n",
        "                # If it's a wall, add collision penalty and stop movement\n",
        "                energy_expenditure += self.energy_loss_stationary + COLLISION_PENALTY\n",
        "                break\n",
        "\n",
        "        return energy_expenditure\n",
        "\n",
        "    def track_energy_movement_angle(self, energy_expenditure, movement, angle_change):\n",
        "        self.energy_tracking.append(-energy_expenditure)  # Negative value for expenditure\n",
        "        self.energy_tracking = self.energy_tracking[-10:]\n",
        "\n",
        "        self.movement_tracking.append(movement)\n",
        "        self.movement_tracking = self.movement_tracking[-10:]\n",
        "\n",
        "        self.angle_tracking.append(angle_change)\n",
        "        self.angle_tracking = self.angle_tracking[-10:]\n",
        "\n",
        "    def _randomize_energy_gradient(self):\n",
        "        self.energy_gradient = np.random.rand(360)\n",
        "        self.predicted_energy_gradient = self.energy_gradient.copy()\n",
        "\n",
        "    def update_efficiency_score(self):\n",
        "        energy_change = self.energy - self.initial_energy\n",
        "        self.efficiency_score = energy_change / max(1, self.step_count)\n",
        "\n",
        "    def get_efficiency_score(self):\n",
        "        return self.efficiency_score\n",
        "\n",
        "    def update_efficiency_bounds(self):\n",
        "        self.min_efficiency = min(self.min_efficiency, self.efficiency_score)\n",
        "        self.max_efficiency = max(self.max_efficiency, self.efficiency_score)\n",
        "\n",
        "    def generate_epsilon(self):\n",
        "        normalized_score = (self.efficiency_score - self.min_efficiency) / (self.max_efficiency - self.min_efficiency + 1e-8)\n",
        "        new_epsilon = 1 - normalized_score\n",
        "        return max(self.epsilon_min, min(self.epsilon_max, new_epsilon))\n",
        "\n",
        "    def _calculate_reward(self):\n",
        "        energy_change = self.energy - self.previous_energy\n",
        "        reward = energy_change * 0.2\n",
        "        if energy_change > 0:\n",
        "            reward += 2\n",
        "        elif energy_change < 0:\n",
        "            reward -= 0.5\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_energy_score(self):\n",
        "        return (self.energy - self.initial_energy) / max(1, self.step_count)\n",
        "\n",
        "    def get_gradient_choice_percentages(self):\n",
        "        total_choices = sum(self.gradient_choices)\n",
        "        if total_choices == 0:\n",
        "            return [0] * self.M3_action_size\n",
        "        return [100 * choices / total_choices for choices in self.gradient_choices]\n",
        "\n",
        "    def load_models(self):\n",
        "        try:\n",
        "            self.M0 = tf.keras.models.load_model(f'M0_{self.color}_model.h5')\n",
        "            self.M1 = tf.keras.models.load_model(f'M1_{self.color}_model.h5')\n",
        "            self.M2 = tf.keras.models.load_model(f'M2_{self.color}_model.h5')\n",
        "            self.M3 = tf.keras.models.load_model(f'M3_{self.color}_model.h5')\n",
        "            self.epsilon_model = tf.keras.models.load_model(f'epsilon_{self.color}_model.h5')\n",
        "            self.window_size_model = tf.keras.models.load_model(f'window_size_{self.color}_model.h5')\n",
        "            return True\n",
        "        except (OSError, IOError):\n",
        "            return False\n",
        "\n",
        "    def get_model_errors(self):\n",
        "        return {\n",
        "            'M0': self.M0_errors[-1] if self.M0_errors else None,\n",
        "            'M1': self.M1_errors[-1] if self.M1_errors else None,\n",
        "            'M2': self.M2_errors[-1] if self.M2_errors else None,\n",
        "            'M3': self.M3_errors[-1] if self.M3_errors else None\n",
        "        }\n",
        "\n",
        "    def get_epsilon(self):\n",
        "        return self.epsilon\n",
        "\n",
        "    def get_window_size(self):\n",
        "        return self.energy_window_size\n",
        "\n",
        "def draw_arrow(screen, color, start, end):\n",
        "    pygame.draw.line(screen, color, start, end, 2)\n",
        "    rotation = math.degrees(math.atan2(start[1]-end[1], end[0]-start[0]))+90\n",
        "    pygame.draw.polygon(screen, color, ((end[0]+5*math.sin(math.radians(rotation)), end[1]+5*math.cos(math.radians(rotation))), (end[0]+5*math.sin(math.radians(rotation-120)), end[1]+5*math.cos(math.radians(rotation-120))), (end[0]+5*math.sin(math.radians(rotation+120)), end[1]+5*math.cos(math.radians(rotation+120)))))\n",
        "\n",
        "def draw_energy_gradient(screen, agent):\n",
        "    center = (int(agent.pos[0] * CELL_SIZE + CELL_SIZE/2), int(agent.pos[1] * CELL_SIZE + CELL_SIZE/2))\n",
        "\n",
        "    # Use the combined gradient from the _locomotion_taxis method\n",
        "    m3_input = np.array(agent.M3_sequence).reshape(1, agent.M3_sequence_length, agent.M3_state_size)\n",
        "    q_values = agent.M3.predict(m3_input, verbose=0)[0]\n",
        "    normalized_q_values = (q_values - np.min(q_values)) / (np.max(q_values) - np.min(q_values) + 1e-8)\n",
        "\n",
        "    random_gradient = np.random.rand(360)\n",
        "\n",
        "    combined_gradient = (\n",
        "        agent.energy_gradient * (1 - agent.epsilon) * 0.4 +\n",
        "        random_gradient * agent.epsilon * 0.6 +\n",
        "        agent._q_values_to_gradient(normalized_q_values) * (1 - agent.epsilon) * 0.6\n",
        "    )\n",
        "\n",
        "    max_energy = np.max(combined_gradient)\n",
        "    if max_energy > 0:\n",
        "        normalized_gradient = combined_gradient / max_energy\n",
        "        base_color = pygame.Color(agent.color)\n",
        "\n",
        "        for angle in range(0, 360, 10):  # Draw every 10 degrees for better performance\n",
        "            energy = normalized_gradient[angle]\n",
        "            end_x = center[0] + int(50 * energy * math.cos(math.radians(angle)))\n",
        "            end_y = center[1] + int(50 * energy * math.sin(math.radians(angle)))\n",
        "            color = (max(0, min(255, int(base_color.r * energy))),\n",
        "                     max(0, min(255, int(base_color.g * energy))),\n",
        "                     max(0, min(255, int(base_color.b * energy))))\n",
        "            pygame.draw.line(screen, color, center, (end_x, end_y), 1)\n",
        "\n",
        "def inference_main():\n",
        "    pygame.init()\n",
        "    screen = pygame.display.set_mode((SCREEN_SIZE, SCREEN_SIZE))\n",
        "    clock = pygame.time.Clock()\n",
        "\n",
        "    agents = []\n",
        "    shared_category_space = SharedCategorySpace()\n",
        "    for i, color in enumerate(COLORS[:N_AGENTS]):\n",
        "        while True:\n",
        "            x, y = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)\n",
        "            if MAZE[y, x] == 0:\n",
        "                agent = Agent(x, y, color)\n",
        "                if agent.load_models():\n",
        "                    print(f\"Loaded existing models for Agent {i+1} ({color})\")\n",
        "                    agents.append(agent)\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"No existing models found for Agent {i+1} ({color}). Cannot run inference.\")\n",
        "                    return\n",
        "\n",
        "    food_positions = {}\n",
        "\n",
        "    def add_food(count):\n",
        "        for _ in range(count):\n",
        "            while True:\n",
        "                x, y = random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)\n",
        "                if MAZE[y, x] == 0 and (x, y) not in food_positions:\n",
        "                    food_positions[(x, y)] = random.randint(FOOD_ENERGY_RANGE[0], FOOD_ENERGY_RANGE[1])\n",
        "                    break\n",
        "\n",
        "    # Initialize food\n",
        "    add_food(CONSTANT_FOOD_COUNT)\n",
        "\n",
        "    running = True\n",
        "    step = 0\n",
        "    while running:\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                running = False\n",
        "\n",
        "        screen.fill((12, 12, 12))\n",
        "\n",
        "        # Draw maze\n",
        "        for y in range(GRID_SIZE):\n",
        "            for x in range(GRID_SIZE):\n",
        "                if MAZE[y, x] == 1:\n",
        "                    pygame.draw.rect(screen, (32, 32, 32), (x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
        "\n",
        "        for agent in agents:\n",
        "            agent.perceive(food_positions, agents, shared_category_space)\n",
        "            agent.move(food_positions, agents, shared_category_space)\n",
        "\n",
        "        # Maintain constant food count\n",
        "        food_deficit = CONSTANT_FOOD_COUNT - len(food_positions)\n",
        "        if food_deficit > 0:\n",
        "            add_food(food_deficit)\n",
        "\n",
        "        # Draw food\n",
        "        for (x, y), energy in food_positions.items():\n",
        "            pygame.draw.circle(screen, (128, 128, 128), (int(x * CELL_SIZE + CELL_SIZE/2), int(y * CELL_SIZE + CELL_SIZE/2)), 3)\n",
        "\n",
        "        # Draw agents and their energy gradients\n",
        "        for agent in agents:\n",
        "            draw_energy_gradient(screen, agent)\n",
        "            start_pos = (int(agent.pos[0] * CELL_SIZE + CELL_SIZE/2), int(agent.pos[1] * CELL_SIZE + CELL_SIZE/2))\n",
        "            end_pos = (int(start_pos[0] + 10 * math.cos(math.radians(agent.angle))),\n",
        "                       int(start_pos[1] + 10 * math.sin(math.radians(agent.angle))))\n",
        "            draw_arrow(screen, pygame.Color(agent.color), start_pos, end_pos)\n",
        "\n",
        "        pygame.display.flip()\n",
        "        clock.tick(24)\n",
        "\n",
        "        step += 1\n",
        "        if step % 100 == 0:\n",
        "            print(f\"\\nInference step: {step}\")\n",
        "            for i, agent in enumerate(agents):\n",
        "                efficiency_score = agent.get_efficiency_score()\n",
        "                gradient_percentages = agent.get_gradient_choice_percentages()\n",
        "                model_errors = agent.get_model_errors()\n",
        "                print(f\"Agent {i+1} ({agent.color}):\")\n",
        "                print(f\"  Efficiency Score: {efficiency_score:.4f}\")\n",
        "                print(f\"  Epsilon: {agent.get_epsilon():.4f}\")\n",
        "                print(f\"  Energy Window Size: {agent.get_window_size()}\")\n",
        "                print(f\"  Gradient Choices:\")\n",
        "                for j, percentage in enumerate(gradient_percentages):\n",
        "                    print(f\"    State {j}: {percentage:.2f}%\")\n",
        "                print(\"  Model Losses:\")\n",
        "                for model, error in model_errors.items():\n",
        "                    print(f\"    {model} Loss: {error if error is not None else 'N/A'}\")\n",
        "\n",
        "    pygame.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference_main()"
      ],
      "metadata": {
        "id": "5NCLJWguVBah"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1cqWPS2Cno4FBHlW2CHyXI3PwQMK8bSP_",
      "authorship_tag": "ABX9TyP/9T6ThWsK4+J9YIZd79lj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}